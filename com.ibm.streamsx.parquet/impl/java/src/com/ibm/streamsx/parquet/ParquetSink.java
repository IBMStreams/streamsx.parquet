/* Generated by Streams Studio: July 20, 2014 9:47:24 PM GMT+03:00 */
package com.ibm.streamsx.parquet;

  
import static parquet.hadoop.ParquetWriter.DEFAULT_BLOCK_SIZE;
import static parquet.hadoop.ParquetWriter.DEFAULT_PAGE_SIZE;

import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.List;
import java.util.Set;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.io.ArrayWritable;
import org.apache.hadoop.io.BooleanWritable;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Writable;
import org.apache.log4j.Logger;

import parquet.column.ParquetProperties.WriterVersion;
import parquet.hadoop.ParquetWriter;
import parquet.hadoop.metadata.CompressionCodecName;
import parquet.hive.writable.BinaryWritable;
import parquet.hive.write.DataWritableWriteSupport;
import parquet.io.api.Binary;
import parquet.schema.MessageType;
import parquet.schema.MessageTypeParser;

import com.ibm.streams.operator.AbstractOperator;
import com.ibm.streams.operator.OperatorContext;
import com.ibm.streams.operator.OperatorContext.ContextCheck;
import com.ibm.streams.operator.StreamingData.Punctuation;
import com.ibm.streams.operator.StreamingInput;
import com.ibm.streams.operator.Tuple;
import com.ibm.streams.operator.Type;
import com.ibm.streams.operator.compile.OperatorContextChecker;
import com.ibm.streams.operator.metrics.Metric;
import com.ibm.streams.operator.metrics.Metric.Kind;
import com.ibm.streams.operator.model.CustomMetric;
import com.ibm.streams.operator.model.InputPortSet;
import com.ibm.streams.operator.model.InputPortSet.WindowMode;
import com.ibm.streams.operator.model.InputPortSet.WindowPunctuationInputMode;
import com.ibm.streams.operator.model.InputPorts;
import com.ibm.streams.operator.model.Libraries;
import com.ibm.streams.operator.model.Parameter;
import com.ibm.streams.operator.model.PrimitiveOperator;
import com.ibm.streams.operator.model.SharedLoader;
import com.ibm.streams.operator.types.Blob;
import com.ibm.streams.operator.types.RString;



/**
 * Class for an operator that consumes tuples and does not produce an output stream. 
 * This pattern supports a number of input streams and no output streams. 
 * <P>
 * The following event methods from the Operator interface can be called:
 * </p>
 * <ul>
 * <li><code>initialize()</code> to perform operator initialization</li>
 * <li>allPortsReady() notification indicates the operator's ports are ready to process and submit tuples</li> 
 * <li>process() handles a tuple arriving on an input port 
 * <li>processPuncuation() handles a punctuation mark arriving on an input port 
 * <li>shutdown() to shutdown the operator. A shutdown request may occur at any time, 
 * such as a request to stop a PE or cancel a job. 
 * Thus the shutdown() may occur while the operator is processing tuples, punctuation marks, 
 * or even during port ready notification.</li>
 * </ul>
 * <p>With the exception of operator initialization, all the other events may occur concurrently with each other, 
 * which lead to these methods being called concurrently by different threads.</p> 
 */
@PrimitiveOperator(name="ParquetSink", namespace="com.ibm.streamsx.parquet",
description="Java Operator ParquetSink",vmArgs={"-Xmx2048m"})
@InputPorts({@InputPortSet(description="Port for tuples ingestion", cardinality=1, optional=false, windowingMode=WindowMode.NonWindowed, windowPunctuationInputMode=WindowPunctuationInputMode.Oblivious), @InputPortSet(description="Optional input ports", optional=true, windowingMode=WindowMode.NonWindowed, windowPunctuationInputMode=WindowPunctuationInputMode.Oblivious)})
@Libraries({"impl/lib/*", "opt/downloaded/*"})
@SharedLoader(value=true)     
public class ParquetSink extends AbstractOperator {
	
	/*
	 * Operator parameters
	 */
	private String hdfsUri = null;
	private String hdfsUser = null;
	private String rootPath = null;
	private int blockSize = DEFAULT_BLOCK_SIZE;
	private int pageSize = DEFAULT_PAGE_SIZE;
	private int dictPageSize = ParquetConstants.DEFAULT_DICTIONARY_PAGE_SIZE;
	private boolean closeOnPunct = false;
	private CompressionCodecName compressionType = CompressionCodecName.UNCOMPRESSED;
	private String compressionTypeStr = CompressionCodecName.UNCOMPRESSED.toString();
	private boolean enableDictionaryEncoding = false;
	private boolean enableSchemaValidation = false;
	private String file = null;
	private boolean overwrite = false;
	private WriterVersion writerVersion = WriterVersion.PARQUET_1_0;
	private int tuplesPerFile = ParquetConstants.DEFAULT_TUPLES_PER_FILE;
	// partition keys list 
	private List<String> partitionKeyNames;  
//	private List<Attribute> partitionKeyAttrs;
	private List<String> partitionValueAttrNames;
	private boolean skipPartitionAttrs = true;

	/*
     * Metrics 
     */    
	private Metric avgFileWriteTime;
	private Metric maxFileWriteTime;
	private Metric minFileWriteTime;
	// closed files number
	private Metric nClosedFiles;
	// opened files number
	private Metric nOpenedFiles;
	// total file processing time
	private long totalFileProcTime = 0;
	// file processing start time stamp 
	private long fileProcStartTimestamp = 0;
		
	/*
	 * Auxiliary members
	 */
	private MessageType parquetSchema = null;
//	private ParquetWriter writer = null;
	private HashMap<Path, ParquetWriterMapValue> writerMap = new HashMap<Path, ParquetWriterMapValue>();
	private Configuration config = null;
	private Writable[] writableValues = null;
	private ArrayWritable arrayWritable = new ArrayWritable(Writable.class, null);
//	private int currTuplesInFile = 0;
	private int outFileIndex = 0;
	private FileSystem fs = null;
	
	
	// auxiliry class for atribute metadata extraction
	private AttrMetadataExtractor attrMetadataExtractor;
		
	/**
	 * Logger
	 */
	private final static Logger logger = Logger.getLogger(ParquetSink.class.getName());		

	
	/**
     * Initialize this operator. Called once before any tuples are processed.
     * @param context OperatorContext for this operator.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
	@Override
	public synchronized void initialize(OperatorContext context)
			throws Exception {

		// Must call super.initialize(context) to correctly setup an operator.
		super.initialize(context);
       
		logger.trace("Operator " + context.getName() + " initializing in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
        
		// initialize auxiliary class for input 
		// tuple attributes metadata extraction
		attrMetadataExtractor = new AttrMetadataExtractor(context);
		
		// initialize metrics
		avgFileWriteTime.setValue(0);
		maxFileWriteTime.setValue(0);
		minFileWriteTime.setValue(0);
		nOpenedFiles.setValue(0);
		nClosedFiles.setValue(0);
		   
		// HDFS client configuration
		config = new Configuration();	
		
		//config.set("fs.defaultFS", getHdfsUri());
		fs = FileSystem.get(new URI(getHdfsUri()),config,  getHdfsUser());
		
		// set username
		System.setProperty("HADOOP_USER_NAME",  getHdfsUser());
		
		//fs =  FileSystem.get(config);
		
		compressionType  = CompressionCodecName.valueOf(compressionTypeStr);
    	// generate schema from an output tuple format
   	 	String parquetSchemaStr = ParquetSchemaGenerator.getInstance().generateParquetSchema(context, partitionValueAttrNames);
    	parquetSchema = MessageTypeParser.parseMessageType(parquetSchemaStr); 
    	
    	int writablesNum = skipPartitionAttrs && partitionKeyNames != null ? attrMetadataExtractor.getAttrCount() - partitionKeyNames.size() : attrMetadataExtractor.getAttrCount();    	
    	logger.trace("Number of attributes to write is " + writablesNum + " out of " + attrMetadataExtractor.getAttrCount());
    	writableValues = new Writable[writablesNum];
    	    	        
        DataWritableWriteSupport.setSchema(parquetSchema, config);       	

        // the file might be opened here if the path
        // generation logic is not tuple specific
        if (!partitionSupportRequired()) {

        	Path outFilePath = generatePath(getHdfsUri(), getRootPath(), getFile());

        	logger.trace("Partition support not required. Using a static output folder '" + outFilePath + "'");

        	// remove output file is necessary
            if (getOverwrite()) {
            	recusivelyRemoveFile(outFilePath);
            }

	    	ParquetWriter<ArrayWritable> writer = openFile(outFilePath);
	    	writerMap.put(outFilePath, new ParquetWriterMapValue(writer, 0));

        } else {       	
        	logger.trace("Partition support required. Using a dynamic (tuple data based) output folder generation mechanism.");
        }
           
	}
	 
	
	private Path generatePath(String hdfsURI, String rootPath, String file) {
		
		return new Path(hdfsURI + rootPath + ParquetConstants.PATH_DELIMITER + file);
	}
	
	private Path generatePartitionedFilePath(Path tupleOutFileFolder, String fileName) {
		
		return new Path(tupleOutFileFolder + ParquetConstants.PATH_DELIMITER + fileName);
	}


	/**
	 * Check params and ports at compile time
	 * @param checker
	 * @throws Exception
	 */
	@ContextCheck(compile = true)
	public static void checkParamsAndPortsAtCompile(OperatorContextChecker checker)
			throws Exception {
		
		/*
		 * Check parameter co-existance constraints
		 */
		checker.checkExcludedParameters(ParquetConstants.CLOSE_ON_PUNCT_PNAME, 
										ParquetConstants.TUPLES_PER_FILE_PNAME);
		checker.checkDependentParameters(ParquetConstants.PARTITION_VALUE_ATTR_NAMES_PNAME, 
				                         ParquetConstants.PARTITION_KEY_NAMES_PNAME);

		/*
		 * Check input/output port number constraints
		 */
		OperatorContext context = checker.getOperatorContext();
		if (context.getNumberOfStreamingInputs() != 1) {
			checker.setInvalidContext( "The operator should have single input port.", null);		
		}

		
	}
	

	
	/**
	 * Check param values at runtime 
	 * @param checker
	 * @throws Exception
	 */
	@ContextCheck(compile = false, runtime = true)
	public static void checkParamsAndPortsAtRuntime(OperatorContextChecker checker)
			throws Exception {
		OperatorContext context = checker.getOperatorContext();
		// "compile" time instance vs the one defined in initialize
		AttrMetadataExtractor attrMetadataExtractor = new AttrMetadataExtractor(context);		
		// list of an input port attribute names
		List<String> portAttrNames = attrMetadataExtractor.getAttrNamesList(0);
		// list of attribute names as specified in parameter value
		List<String> partitionValueAttrNames = context.getParameterValues(ParquetConstants.PARTITION_VALUE_ATTR_NAMES_PNAME);
		List<String> partitionKeyNameNames = context.getParameterValues(ParquetConstants.PARTITION_KEY_NAMES_PNAME);
		if (partitionValueAttrNames.size() != partitionKeyNameNames.size()) {
			checker.setInvalidContext("The number of keys specified in '" + ParquetConstants.PARTITION_KEY_NAMES_PNAME + 
					                  "' operator param should match the number of values specified in '" + 
					                  ParquetConstants.PARTITION_VALUE_ATTR_NAMES_PNAME + "' operator param", null);
		}
		
		for (String partitionValueAttrName: partitionValueAttrNames) {
			if (!portAttrNames.contains(partitionValueAttrName)) { 				
				String errMsg = "Illegal operator configuration: the attribute '"  + partitionValueAttrName + "' specified in  'partitionValueAttrNames' operator parameter is not defined in the input port schema"; 
				checker.setInvalidContext( errMsg, null); 				
			}
		}
	}
	
	/**
     * Notification that initialization is complete and all input and output ports 
     * are connected and ready to receive and submit tuples.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */	
    @Override
    public synchronized void allPortsReady() throws Exception {
    	// This method is commonly used by source operators. 
    	// Operators that process incoming tuples generally do not need this notification. 
        OperatorContext context = getOperatorContext();
        logger.trace("Operator " + context.getName() + " all ports are ready in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
    }

    /**
     * Process an incoming tuple that arrived on the specified port.
     * @param stream Port the tuple is arriving on.
     * @param tuple Object representing the incoming tuple.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
    @Override
    public synchronized void process(StreamingInput<Tuple> stream, Tuple tuple)
            throws Exception {
    	Path staticOutPath = new Path(hdfsUri + ParquetConstants.PATH_DELIMITER + rootPath);
    	// by default generate un-partitioned file path
    	Path outPath = generatePath(getHdfsUri(), getRootPath(), getFile());
    	if (partitionSupportRequired()) {	    	
    		// calculate the tuple out path. The
	    	// path is derived from the tuple data.
	    	Path tupleOutFileFolder = new Path(staticOutPath + ParquetConstants.PATH_DELIMITER +  genOutFileFolder(tuple));
	    	logger.trace("Tuple based partition path '" + tupleOutFileFolder + "'");
	    	outPath = generatePartitionedFilePath(tupleOutFileFolder, file);
	    	// parquet writer for this tuple already exists - no need to create a new one
	    	if (!writerMap.containsKey(outPath)) {
		    	// prepare out folder if necessary  
		    	if (!pathExists(tupleOutFileFolder)) {
			    	logger.trace("Folder '" + tupleOutFileFolder + "' doesn't exists. Creating.");
		    		createFolder(tupleOutFileFolder, ParquetConstants.DEFAULT_FOLDER_PERMISSIONS);	    		
		    	}
		    	// remove old file if necessary
		    	if (pathExists(outPath) && getOverwrite()) {	    		
					recusivelyRemoveFile(outPath);
		    	}
		    	logger.trace("About to open file '" + outPath + "' for tuple '" + tuple + "'");
		    	ParquetWriter<ArrayWritable> writer = openFile(outPath);
		    	writerMap.put(outPath, new ParquetWriterMapValue(writer, 0));
    		}
    	} 
    	
    	List<Type> attrTypeList = attrMetadataExtractor.getAttrTypesList(stream);
    	List<String> attrNamesList = attrMetadataExtractor.getAttrNamesList(stream);
    	
    	for (int i = 0; i <  attrMetadataExtractor.getAttrCount(); i++) {    		    		
    		Type attrType = attrTypeList.get(i);    		
    		Object attrValue = tuple.getObject(i);    		
    		if (skipPartitionAttrs) {
    			String attrName = (String)attrNamesList.get(i);
    			if (partitionValueAttrNames != null && !partitionValueAttrNames.contains(attrName)) {
        			writableValues[i] = SPLPrimitiveWritableObj(attrType, attrValue);    				
    			}
     		}
    		else {
    			writableValues[i] = SPLPrimitiveWritableObj(attrType, attrValue);
    		}
    	}    	
    	
    	arrayWritable.set(writableValues);
    	ParquetWriterMapValue pwv = ((ParquetWriterMapValue)writerMap.get(outPath));    	
    	pwv.pw.write(arrayWritable);    	
		pwv.writerTupleCount++;
		// close the current file and open a new one
		// if tuplesPerFile parameter defined. 
		if (getTuplesPerFile() > 0 && (pwv.writerTupleCount == tuplesPerFile)) {
			closeFile(outPath);
			writerMap.remove(outPath);

			// update metric
			outFileIndex++;
			
			Path nextFilePath = new Path(outPath.toString() + ParquetConstants.FILE_INDEX_DELIMITER + outFileIndex);			
			if (getOverwrite()) {
				recusivelyRemoveFile(nextFilePath);
		    }
			//openFile(nextFilePath);
			ParquetWriter<ArrayWritable> writer = openFile(nextFilePath);
	    	writerMap.put(outPath, new ParquetWriterMapValue(writer, 0));
		}
    }
    



	/**
     * Process an incoming punctuation that arrived on the specified port.
     * @param stream Port the punctuation is arriving on.
     * @param mark The punctuation mark
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
    @Override
    public void processPunctuation(StreamingInput<Tuple> stream,
    		Punctuation mark) throws Exception {
    	super.processPunctuation(stream, mark);
//    	if (getCloseOnPunct()) {
//    		closeFile();
//    		refreshOutPath();
//    		if (getOverwrite()) {
//    			recusivelyRemoveFile(outFilePath);
//		    }
//			openFile(outFilePath);
//    	}
    }

    /**
     * Shutdown this operator.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
    @Override
    public synchronized void shutdown() throws Exception {
        OperatorContext context = getOperatorContext();
        logger.trace("Operator " + context.getName() + " shutting down in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
        
        for (Path outPath: writerMap.keySet()) {
        	 closeFile(outPath); 	
        }
       
        writerMap.clear();
        
        // Must call super.shutdown()
        super.shutdown();
    }

    /*
     * Parameter definitions
     */
    
    @Parameter(name = "blockSize", description = "", optional = true)
    public void setBlockSize(int blockSize) {
    	this.blockSize  = blockSize;
    }

    public int getBlockSize() {
    	return this.blockSize;
    }

    @Parameter(name = "pageSize", description = "", optional = true)
    public void setPageSize(int pageSize) {
    	this.pageSize  = pageSize;
    }

    public int getPageSize() {
    	return this.pageSize;
    }
  
    @Parameter(name = "hdfsUri", description = "", optional = false)
    public void setHdfsUri(String hdfsUri) {
    	this.hdfsUri  = hdfsUri;
    }

    public String getHdfsUri() {
    	return this.hdfsUri;
    }
    
    @Parameter(name = "hdfsUser", description = "", optional = false)
    public void setUser(String hdfsUser) {
    	this.hdfsUser  = hdfsUser;
    }

    public String getHdfsUser() {
    	return this.hdfsUser;
    }
    
    @Parameter(name = "rootPath", description = "", optional = false)
    public void setRootPath(String rootPath) {
    	this.rootPath  = rootPath;
    }

    public String getRootPath() {
    	return this.rootPath;
    }
    
    @Parameter(name = "file", description = "", optional = false)
    public void setFile(String file) {
    	this.file  = file;
    }

    public String getFile() {
    	return this.file;
    }

	@Parameter(name = "dictPageSize", description = "", optional = true)
	public void setDictPageSize(int dictPageSize) {
		this.dictPageSize = dictPageSize;
	}

	 public int getDictPageSize() {
		 return this.dictPageSize;
	 }
	 
	 
     @Parameter(name = "closeOnPunct", description = "", optional = true)
     public void setCloseOnPunct(boolean closeOnPunct) {
    	 this.closeOnPunct  = closeOnPunct;
     }

     public boolean getCloseOnPunct() {
    	 return this.closeOnPunct;
     }
 
     @Parameter(name = "compressionType", description = "", optional = true)
     public void setCompressionType(String compressionTypeStr) {
    	 this.compressionTypeStr  = compressionTypeStr.toUpperCase();
     }

     public String getCompressionType() {
    	 return this.compressionTypeStr;
     }


     @Parameter(name = "enableDictionaryEncoding", description = "", optional = true)
 	 public void setEnableDictionaryEncoding(boolean enableDictionaryEncoding) {
 		this.enableDictionaryEncoding = enableDictionaryEncoding;
 	 }

     public boolean getEnableDictionaryEncoding() {
    	 return enableDictionaryEncoding;
     }
     

     @Parameter(name = "enableSchemaValidation", description = "", optional = true)
     public void setEnableSchemaValidation(boolean enableSchemaValidation) {
    	 this.enableSchemaValidation = enableSchemaValidation;
     }

     public boolean getEnableSchemaValidation() {
 		return enableSchemaValidation;
 	 }

     @Parameter(name = "overwrite", description = "", optional = true)
     public void setOverwrite(boolean overwrite) {
    	 this.overwrite = overwrite;
     }

     public boolean getOverwrite() {
 		return this.overwrite;
 	 }
 	

     @Parameter(name = "writerVersion", description = "", optional = true)
     public void setWriterVersion(String writerVersion) {
        this.writerVersion = WriterVersion.valueOf(writerVersion);
     }

     public WriterVersion getWriterVersion() {
    	 return this.writerVersion;
     }

     @Parameter(name = "tuplesPerFile", description = "", optional = true)
     public void setTuplesPerFile(int tuplesPerFile) {
        this.tuplesPerFile = tuplesPerFile;
     }

     public int getTuplesPerFile() {
    	 return this.tuplesPerFile;
     }
     
     
     @Parameter(name = "partitionKeyNames", description = "", optional = true, cardinality = -1)
 	public void setPartitionKeyNames(List<String> partitionKeyNames) {
    	logger.trace("Set partition key names to '" + partitionKeyNames + "'");    	
    	this.partitionKeyNames = partitionKeyNames;
 	}

 	 public List<String> getPartitionKeyNames() {
 		 return this.partitionKeyNames;
 	 }
 	 
 	 @Parameter(name = "partitionValueAttrNames", description = "", optional = true, cardinality = -1)
 	 public void setPartitionValueAttrNames(List<String> attrNames) {
 		partitionValueAttrNames = attrNames;
 	 }
 	
 	 public List<String> getPartitionValueAttrNames() {
 		 return this.partitionValueAttrNames;
 	 }

 	 @Parameter(name = "skipPartitionAttrs", description = "", optional = true)
 	 public void setSkipPartitionAttrs(boolean skipPartitionAttrs) {
 		this.skipPartitionAttrs = skipPartitionAttrs;
 	 }
 	
 	 public boolean getSkipPartitionAttrs() {
 		 return this.skipPartitionAttrs;
 	 }


     /*
      * Metric definitions 
      */
     
     @CustomMetric(description="Average file write time", kind=Kind.COUNTER)
     public void setavgFileWriteTime(Metric avgFileWriteTime) {
         this.avgFileWriteTime = avgFileWriteTime;
     }

     public Metric getavgFileWriteTime() {
         return avgFileWriteTime;
     }

     @CustomMetric(description="Max file write time", kind=Kind.COUNTER)
     public void setmaxFileWriteTime(Metric maxFileWriteTime) {
         this.maxFileWriteTime = maxFileWriteTime;
     }

     public Metric getmaxFileWriteTime() {
         return maxFileWriteTime;
     }

     @CustomMetric(description="Min file write time", kind=Kind.COUNTER)
     public void setminFileWriteTime(Metric minFileWriteTime) {
         this.minFileWriteTime = minFileWriteTime;
     }

     public Metric getminFileWriteTime() {
         return minFileWriteTime;
     }

     @CustomMetric(description="Closed files num", kind=Kind.COUNTER)
     public void setnClosedFiles(Metric nClosedFiles) {
         this.nClosedFiles = nClosedFiles;
     }

     public Metric getnClosedFiles() {
         return nClosedFiles;
     }

     @CustomMetric(description="Opened files num", kind=Kind.COUNTER)
     public void setnOpenedFiles(Metric nOpenedFiles) {
         this.nOpenedFiles = nOpenedFiles;
     }

     public Metric getnOpenedFiles() {
         return nClosedFiles;
     }
   

     /*
      * Auxiliary methods
      */
     
     private Writable SPLPrimitiveWritableObj(Type attrType, Object value) {
 		switch (attrType.getMetaType()) {
 			case BOOLEAN: {
 				Boolean cValue = (Boolean)value;
 				return new BooleanWritable(cValue.booleanValue());
 			}
 			case INT32: {
 				Integer cValue = (Integer)value;
 				return new IntWritable(cValue.intValue());
 			}
 			case INT64: {
 				Long cValue = (Long)value;
 				return new LongWritable(cValue.longValue());
 			}
 			case FLOAT32: {
 				Float cValue = (Float)value;
 				return new FloatWritable(cValue.floatValue());
 			}
 			case FLOAT64: {
 				Double cValue = (Double)value;
 				return new DoubleWritable(cValue.doubleValue());
 			}
 			case RSTRING: {
 				RString cValue = (RString)value;
 				return new BinaryWritable(Binary.fromByteArray(cValue.getData()));
 			}
 			case BLOB: {
 				Blob cValue = (Blob)value;
 				return new BinaryWritable(Binary.fromByteArray(cValue.getByteBuffer().array()));
 			}
 			default: {	
 				Blob cValue = (Blob)value;
 				return new BinaryWritable(Binary.fromByteArray(cValue.getByteBuffer().array()));
 			}
 		}
 	}
	
     /**
      * Recursively create folder in HDFS if necessary
      */
     private void createFolder(Path outFilePath, FsPermission fsPermission) throws IOException {		
 		try { 			 			
 			if (!fs.exists(outFilePath))  {
 				FileSystem.mkdirs(fs, outFilePath, fsPermission);
 			}
 		} catch (IOException ioe) {
 			logger.error("Failed to remove file '" + outFilePath.toUri().toString() + "'\n");
 			throw ioe;
 		}
 	}


 	private boolean pathExists(Path tupleOutFileFolder) throws IOException { 		
 		return fs.exists(tupleOutFileFolder);
 	}


    /**
     * Recursively remove file in HDFS 
     */
    private void recusivelyRemoveFile(Path outFilePath) throws IOException {		
		try {				
			if (fs.exists(outFilePath))  {
				fs.delete(outFilePath, true);
			}
		} catch (IOException ioe) {
			logger.error("Failed to remove file '" + outFilePath.toUri().toString() + "'\n");
			throw ioe;
		}
	}


	private ParquetWriter<ArrayWritable> openFile(Path outFilePath) throws IOException {			
		logger.trace("About to initialize parquet write for file '" + outFilePath.toString() + "' with: " +
        			 "compression type '" + getCompressionType() + "'," +
        			 "block size '" + getBlockSize() + "'," +
        			 "page size '" + getPageSize() + "'," +
        			 "dictionary page size '" + getDictPageSize() + "'");
				
		// update metric state variables    
        fileProcStartTimestamp = System.currentTimeMillis();			 
        nOpenedFiles.incrementValue(1l);
        
        return ParquetWriterFactory.getInstance().createArrayWritableWriter(
        		outFilePath, 
        		this.parquetSchema.toString(), 
        		this.compressionType, 
        		getBlockSize(), 
        		getPageSize(), 
        		getDictPageSize(), 
        		getEnableDictionaryEncoding(), 
        		getEnableSchemaValidation(), 
        		WriterVersion.PARQUET_1_0);
       
        
	}


	private void closeFile(Path outFilePath) throws IOException {
		ParquetWriter<ArrayWritable> pw = writerMap.get(outFilePath).pw;
		if (pw != null) {
			pw.close();
			//writerMap.remove(outFilePath);
        }        
        
		nClosedFiles.incrementValue(1l);
		long currFileProcValue = System.currentTimeMillis() - fileProcStartTimestamp;
        totalFileProcTime += currFileProcValue;        
		avgFileWriteTime.setValue(totalFileProcTime/nClosedFiles.getValue());
		
		if (maxFileWriteTime.getValue() < currFileProcValue) { 
			maxFileWriteTime.setValue(currFileProcValue);
		}
			
		if ((minFileWriteTime.getValue() > currFileProcValue) || (minFileWriteTime.getValue() == 0)) {
			minFileWriteTime.setValue(currFileProcValue);
		}
	}
	
//	private void refreshOutPath() {
//		outFilePath = generatePath(getHdfsUri(), getRootPath(), getFile() + "." + outFileIndex);
//	}
//	
	 


	/**
	 * @return - true if partition support is required
	 * as defined by operator configuration.
	 */
	private boolean partitionSupportRequired() {
		return partitionKeyNames != null && partitionKeyNames.size() > 0;
	}

			
			
	private String genOutFileFolder(Tuple tuple) {
		StringBuffer resBuf = new StringBuffer();
		for (int i = 0; i < partitionValueAttrNames.size(); i++) {
			if (i > 0) {
				resBuf.append(ParquetConstants.PATH_DELIMITER);
			}
			resBuf.append((String)partitionKeyNames.get(i) + ParquetConstants.PARTITION_NVP_DELIMITER + tuple.getString((String)partitionValueAttrNames.get(i)));				
		}
		
		return resBuf.toString();
		
	}

	
	class ParquetWriterMapValue {
		ParquetWriter<ArrayWritable> pw;
		int writerTupleCount = 0;
		
		ParquetWriterMapValue(ParquetWriter<ArrayWritable> pw, int writerTupleCount) {
			this.pw = pw;
			this.writerTupleCount = writerTupleCount;
		}
	}
}
			