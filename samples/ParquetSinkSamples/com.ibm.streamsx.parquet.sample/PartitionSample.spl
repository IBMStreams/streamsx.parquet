namespace com.ibm.streamsx.parquet.sample;

use com.ibm.streamsx.parquet::ParquetSink ;

/**
 * The sample demonstrates ParquetSink
 * utilization for timestamp-based partition.
 * The output files are closed based on tuple count.
 * 
 */
composite PartitionCloseOnCountSample {

	param
		expression<rstring> $hdfsUri: getSubmissionTimeValue("hdfsUri", "hdfs://9.148.58.232:8020");
		expression<rstring> $hdfsUser : getSubmissionTimeValue("hdfsUser", "hdfs");
		expression<rstring> $hdfsRootPath: getSubmissionTimeValue("hdfsRootPath", "/user/hdfshome/parquet/partitioned");
	         
	graph
		

		/**
		 * Read and Parse raw events
		 */		
		stream<CompressionSample.RawNetworkEvent_t> NetworkEvent = FileSource()
		{
			param
				file: "partitionSampleData.txt";
				format: csv;
				separator: "|";
		}

		/**
		 * Add partition-specific attributes
		 */
		stream<NetworkEvent, tuple<int32 YEAR, int32 MONTH,  int32 DAY, int32 HOUR>> PartitionedNetworkEvent = Functor(NetworkEvent) {
			logic 
				state: {
					mutable timestamp dt_orig_ts;
				}
				onTuple NetworkEvent: {
					// original format 2014-07-28 12:42:45.618
					dt_orig_ts = toTimestamp(Sys.YYYY_MM_DD_hh_mm_ss_mmm,dt_orig);
				}
			
			output PartitionedNetworkEvent:
            	YEAR = (int32)year(dt_orig_ts), 
            	MONTH = ((int32)month(dt_orig_ts)) + 1, 
            	DAY = (int32)day(dt_orig_ts), 
             	HOUR = (int32)hour(dt_orig_ts);
	
		}


		() as NetworkEventParquetSink = ParquetSink(PartitionedNetworkEvent)
		{
			param
				hdfsUri: $hdfsUri;
				hdfsUser: $hdfsUser;
     	    	rootPath: $hdfsRootPath;
     	        file:  "partitionSampleData.txt";
     	        overwrite: true;
     	        compressionType: "uncompressed";
     	        tuplesPerFile: 5;
               	partitionKeyNames: "year", "month", "day", "hour";
                partitionValueAttrNames: "YEAR", "MONTH", "DAY", "HOUR";    
      	        blockSize: 67108864; 
		}

}
